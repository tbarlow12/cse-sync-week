{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip: integer (nullable = true)\n",
      " |-- app: integer (nullable = true)\n",
      " |-- device: integer (nullable = true)\n",
      " |-- os: integer (nullable = true)\n",
      " |-- channel: integer (nullable = true)\n",
      " |-- click_time: timestamp (nullable = true)\n",
      " |-- attributed_time: timestamp (nullable = true)\n",
      " |-- is_attributed: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0_df = spark.read.csv(\"sample-data/train_sample.csv\",header=True,inferSchema=True)\n",
    "t0_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Schema\n",
    "- ip - IP address of click\n",
    "- app - app id for marketing\n",
    "- device - device **type** id of user mobile phone (e.g. iphone 6, iphone 7, etc.)\n",
    "- os - os version id of user mobile phone\n",
    "- channel - channel id of mobile ad publisher\n",
    "- click_time - timestamp of click (UTC)\n",
    "- attributed_time - if user downloaded the app after clicking an ad, this is the time of the app download\n",
    "- is_attributed - the target that is to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip: integer (nullable = true)\n",
      " |-- app: integer (nullable = true)\n",
      " |-- device: integer (nullable = true)\n",
      " |-- os: integer (nullable = true)\n",
      " |-- channel: integer (nullable = true)\n",
      " |-- click_time: timestamp (nullable = true)\n",
      " |-- attributed_time: timestamp (nullable = true)\n",
      " |-- is_attributed: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "#Extract day and hour from click time\n",
    "t0_df = t0_df.withColumn(\"day\",dayofmonth(t0_df[\"click_time\"]))\n",
    "t0_df = t0_df.withColumn(\"hour\",hour(t0_df[\"click_time\"]))\n",
    "\n",
    "t0_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features added\n",
    "- day - day of month of click\n",
    "- hour - hour of day of click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+-------+-------------------+---------------+-------------+---+----+----------------+----------------+----------------+-----------------+--------------------+\n",
      "|    ip|app|device| os|channel|         click_time|attributed_time|is_attributed|day|hour|          appVec|           osVec|       deviceVec|       channelVec|               ipVec|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+---+----+----------------+----------------+----------------+-----------------+--------------------+\n",
      "| 87540| 12|     1| 13|    497|2017-11-07 09:30:38|           null|            0|  7|   9|(551,[12],[1.0])|(866,[13],[1.0])|(3867,[1],[1.0])|(498,[497],[1.0])|(364757,[87540],[...|\n",
      "|105560| 25|     1| 17|    259|2017-11-07 13:40:27|           null|            0|  7|  13|(551,[25],[1.0])|(866,[17],[1.0])|(3867,[1],[1.0])|(498,[259],[1.0])|(364757,[105560],...|\n",
      "|101424| 12|     1| 19|    212|2017-11-07 18:05:24|           null|            0|  7|  18|(551,[12],[1.0])|(866,[19],[1.0])|(3867,[1],[1.0])|(498,[212],[1.0])|(364757,[101424],...|\n",
      "| 94584| 13|     1| 13|    477|2017-11-07 04:58:08|           null|            0|  7|   4|(551,[13],[1.0])|(866,[13],[1.0])|(3867,[1],[1.0])|(498,[477],[1.0])|(364757,[94584],[...|\n",
      "| 68413| 12|     1|  1|    178|2017-11-09 09:00:09|           null|            0|  9|   9|(551,[12],[1.0])| (866,[1],[1.0])|(3867,[1],[1.0])|(498,[178],[1.0])|(364757,[68413],[...|\n",
      "| 93663|  3|     1| 17|    115|2017-11-09 01:22:13|           null|            0|  9|   1| (551,[3],[1.0])|(866,[17],[1.0])|(3867,[1],[1.0])|(498,[115],[1.0])|(364757,[93663],[...|\n",
      "| 17059|  1|     1| 17|    135|2017-11-09 01:17:58|           null|            0|  9|   1| (551,[1],[1.0])|(866,[17],[1.0])|(3867,[1],[1.0])|(498,[135],[1.0])|(364757,[17059],[...|\n",
      "|121505|  9|     1| 25|    442|2017-11-07 10:01:53|           null|            0|  7|  10| (551,[9],[1.0])|(866,[25],[1.0])|(3867,[1],[1.0])|(498,[442],[1.0])|(364757,[121505],...|\n",
      "|192967|  2|     2| 22|    364|2017-11-08 09:35:17|           null|            0|  8|   9| (551,[2],[1.0])|(866,[22],[1.0])|(3867,[2],[1.0])|(498,[364],[1.0])|(364757,[192967],...|\n",
      "|143636|  3|     1| 19|    135|2017-11-08 12:35:26|           null|            0|  8|  12| (551,[3],[1.0])|(866,[19],[1.0])|(3867,[1],[1.0])|(498,[135],[1.0])|(364757,[143636],...|\n",
      "| 73839|  3|     1| 22|    489|2017-11-08 08:14:37|           null|            0|  8|   8| (551,[3],[1.0])|(866,[22],[1.0])|(3867,[1],[1.0])|(498,[489],[1.0])|(364757,[73839],[...|\n",
      "| 34812|  3|     1| 13|    489|2017-11-07 05:03:14|           null|            0|  7|   5| (551,[3],[1.0])|(866,[13],[1.0])|(3867,[1],[1.0])|(498,[489],[1.0])|(364757,[34812],[...|\n",
      "|114809|  3|     1| 22|    205|2017-11-09 10:24:23|           null|            0|  9|  10| (551,[3],[1.0])|(866,[22],[1.0])|(3867,[1],[1.0])|(498,[205],[1.0])|(364757,[114809],...|\n",
      "|114220|  6|     1| 20|    125|2017-11-08 14:46:16|           null|            0|  8|  14| (551,[6],[1.0])|(866,[20],[1.0])|(3867,[1],[1.0])|(498,[125],[1.0])|(364757,[114220],...|\n",
      "| 36150|  2|     1| 13|    205|2017-11-07 00:54:09|           null|            0|  7|   0| (551,[2],[1.0])|(866,[13],[1.0])|(3867,[1],[1.0])|(498,[205],[1.0])|(364757,[36150],[...|\n",
      "| 72116| 25|     2| 19|    259|2017-11-08 23:17:45|           null|            0|  8|  23|(551,[25],[1.0])|(866,[19],[1.0])|(3867,[2],[1.0])|(498,[259],[1.0])|(364757,[72116],[...|\n",
      "|  5314|  2|     1|  2|    477|2017-11-09 07:33:41|           null|            0|  9|   7| (551,[2],[1.0])| (866,[2],[1.0])|(3867,[1],[1.0])|(498,[477],[1.0])|(364757,[5314],[1...|\n",
      "|106598|  3|     1| 20|    280|2017-11-09 03:44:35|           null|            0|  9|   3| (551,[3],[1.0])|(866,[20],[1.0])|(3867,[1],[1.0])|(498,[280],[1.0])|(364757,[106598],...|\n",
      "| 72065| 20|     2| 90|    259|2017-11-06 23:14:08|           null|            0|  6|  23|(551,[20],[1.0])|(866,[90],[1.0])|(3867,[2],[1.0])|(498,[259],[1.0])|(364757,[72065],[...|\n",
      "| 37301| 14|     1| 13|    349|2017-11-06 20:07:00|           null|            0|  6|  20|(551,[14],[1.0])|(866,[13],[1.0])|(3867,[1],[1.0])|(498,[349],[1.0])|(364757,[37301],[...|\n",
      "+------+---+------+---+-------+-------------------+---------------+-------------+---+----+----------------+----------------+----------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Encode categorical features (app, device, os, channel)\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"ip\",\"app\",\"device\",\"os\",\"channel\"],\n",
    "                                outputCols=[\"ipVec\",\"appVec\",\"deviceVec\",\"osVec\",\"channelVec\"])\n",
    "\n",
    "model = encoder.fit(t0_df)\n",
    "t0_df = model.transform(t0_df)\n",
    "t0_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|            features|is_attributed|\n",
      "+--------------------+-------------+\n",
      "|(5784,[12,552,443...|            0|\n",
      "|(5784,[25,552,443...|            0|\n",
      "|(5784,[12,552,443...|            0|\n",
      "|(5784,[13,552,443...|            0|\n",
      "|(5784,[12,552,441...|            0|\n",
      "|(5784,[3,552,4435...|            0|\n",
      "|(5784,[1,552,4435...|            0|\n",
      "|(5784,[9,552,4443...|            0|\n",
      "|(5784,[2,553,4440...|            0|\n",
      "|(5784,[3,552,4437...|            0|\n",
      "|(5784,[3,552,4440...|            0|\n",
      "|(5784,[3,552,4431...|            0|\n",
      "|(5784,[3,552,4440...|            0|\n",
      "|(5784,[6,552,4438...|            0|\n",
      "|(5784,[2,552,4431...|            0|\n",
      "|(5784,[25,553,443...|            0|\n",
      "|(5784,[2,552,4420...|            0|\n",
      "|(5784,[3,552,4438...|            0|\n",
      "|(5784,[20,553,450...|            0|\n",
      "|(5784,[14,552,443...|            0|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "#Select features to actually use in training\n",
    "vectorAssembler = VectorAssembler(inputCols=[\n",
    "    \"appVec\",\n",
    "    \"deviceVec\",\n",
    "    \"osVec\",\n",
    "    \"channelVec\",\n",
    "    \"day\",\n",
    "    \"hour\"\n",
    "], outputCol=\"features\")\n",
    "\n",
    "v_t0_df = vectorAssembler.transform(t0_df)\n",
    "v_t0_df.select(\"features\",\"is_attributed\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60059 39941\n"
     ]
    }
   ],
   "source": [
    "splits = v_final_df.randomSplit([0.6,0.4],1)\n",
    "\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]\n",
    "\n",
    "print(train_df.count(),test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"is_attributed\",featuresCol=\"features\")\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree: 0.5156890871962319\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"is_attributed\",\n",
    "    rawPredictionCol=\"prediction\")\n",
    "\n",
    "dt_accuracy = evaluator.evaluate(dt_predictions)\n",
    "print(\"DecisionTree:\",dt_accuracy)\n",
    "\n",
    "#lr_accuracy = evaluator.evaluate(lr_predictions)\n",
    "#print(\"Logistic Regression:\",lr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = spark.read.csv(\"sample-data/test.csv\",header=True,inferSchema=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
